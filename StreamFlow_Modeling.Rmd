---
title: "StreamFlow Modeling"
author: "Victoria Cutler & Mallory Giesie"
date: "2023-05-10"
output: html_document
---

### Libraries & Sourced Functions

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(sensitivity)
library(tidyverse)
library(lubridate)
library(reldist)
library(purrr)
library(ggpubr)

# source in functions
source("R/cper.R")
source("R/nse.R")
source("R/relerr.R")
source("R/summer_flow_model_performance.R")
```

### 1. Summer Flow Model Performance: Function Example

```{r creating summer stream flow performance metric}
# read in data
sager = read.table("Data/sager.txt", header=T) |>  
  mutate(date = paste(day,month,year, sep="/")) |> 
  mutate(date = as.Date(date, "%d/%m/%Y"))

# summer months definition
summer_months = c(5, 6, 7, 8, 9)

# data wrangling for performance evaluation 
streamflow_data <- sager |> 
  filter(month == summer_months) |> 
  group_by(year) |> 
  summarise(yrly_avg_flow_obs = mean(obs),
            yrly_avg_flow_mod = mean(model))

streamflow_data_model <- streamflow_data |> 
  select(yrly_avg_flow_mod)

streamflow_data_obs <- streamflow_data |> 
  select(yrly_avg_flow_obs)

# performance evaluation
perf_metric <- summer_flow_model_performance(modeled_data = streamflow_data_model, observed_data = streamflow_data_obs)

print(paste0("Please see 'summer_flow_model_performance.R' for documentation on the performance metrics. Using one model, we see a performance metric of: ", round(perf_metric, 2), "."))
```

### 1. Applying Performance Function to a Subset of the Sagehen data set (with multiple simulations) for Calibration

```{r}
# read in data with multiple simulations
# each column is modeled streamflow data where the model uses a different parameter set
msage = read.table("Data/sagerm.txt", header=T)

# rename each column to a simulation id
nsim = ncol(msage)
snames = sprintf("S%d",seq(from=1, to=nsim))
colnames(msage) = snames

# append date information to join observed data
msage$date = sager$date
msage$month = sager$month
msage$year = sager$year
msage$day = sager$day
msage$wy = sager$wy

# add observed data
msage = left_join(msage, sager[,c("obs","date")], by=c("date"))

# subset for split sample calibration
short_msage = subset(msage, wy < 1975)

# compute performance measures for output from all parameters

# selecting just the simulation id columns
res = short_msage %>% select(!c("date","month","year","day","wy","obs")) %>%
      # apply the summer_flow_model_performance() function where each simulation id column is        the first model input (modeled data) and the second input is the observed column and           return a dataframe 
map_df(summer_flow_model_performance, short_msage$obs)

res <- as.data.frame(res)

colnames(res) <- snames

best = res[which.max(res)]

worst = res[which.min(res)]
```

### 2. Summarize the performance over the calibration period in 1-2 graphs; you can decide what is useful

```{r performance visuals}
# plot 1: histogram of model metrics
long_data <- res |>  
  pivot_longer(cols = everything(), 
               names_to = "model", 
               values_to = "metric")

ggplot(long_data, aes(x = metric)) +
  geom_histogram(fill = "lightblue", color = "black") +
  labs(title = "Histogram: Model Performance Values for Predicting Summer Flow",
       x = "Model Performance",
       y = "Count")
```

**Our performance function combines a correlation metric between the modeled and observed data with the cper metric, which combines the Nash-Sutcliffe Efficiency (NSE) metric with relative error. This composite metric yields a performance score ranging from 0 to 1, where a score of 0 represents poor performance and a score of 1 indicates perfect alignment between the modeled and observed data.**

**With our best parameter set, we achieved a performance score of 0.27. This indicates that our model's predictions show some correlation and reasonable agreement with the observed data, but there is still room for improvement to achieve a higher score**

### 3. Record your 'best' and 'worst' parameter set in this [spreadsheet](https://docs.google.com/spreadsheets/d/1444ILTaP6pcqvudQopZZVP2WDDJ1NNgXX6sdPdwdbXE/edit#gid=0) and in your Rmd

```{r}
print("Please see group 'Mallory and Victoria'")
```

### Rubric 

50 pts R function (10pts) combines at least 2 performance metrics (5) function is applied to part of Sagehen data set (5) Calibration (10pts) your function is applied to the msage dataset across all parameter sets (5) your metrics are used to select the best and worst parameter set (5) Graphs (20pts) 1-2 plots of summary of performance over calibration period (5) 1-2 plots of output of acceptable parameter sets that clearly visualize uncertainty (5) plot maximum likelihood estimate for post-calibration period (5) graphing style (axis labels, legibility) (5) Discussion (10pts) short explanation on metrics used (5) 1-2 sentences on calibration and post-calibration uncertainty analysis
